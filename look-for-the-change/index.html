<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<title>Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos | CVPR 2022</title>
<meta property="og:title" content="Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://soczech.github.io/look-for-the-change/" />
<meta property="og:url" content="https://soczech.github.io/look-for-the-change/" />
<meta property="og:site_name" content="Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos" />

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#002d56">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css">
    
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png">
    <link rel="shortcut icon" href="/assets/favicon/favicon.ico">

  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos</h1>
      <h2 class="conference">CVPR 2022</h2>
      <h2 class="project-tagline"><a href="https://scholar.google.com/citations?user=sJdrqpUAAAAJ">Tomáš&nbsp;Souček</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=_VmflIEAAAAJ">Jean-Baptiste&nbsp;Alayrac</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=9tfacCoAAAAJ">Antoine&nbsp;Miech</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=-9ifK0cAAAAJ">Ivan&nbsp;Laptev</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=NCtKHnQAAAAJ">Josef&nbsp;Sivic</a></h2>
        <a href="https://arxiv.org/abs/2203.11637" class="btn">Paper</a>
        <a href="https://github.com/soCzech/LookForTheChange" class="btn">Code</a>
        <a href="https://github.com/soCzech/ChangeIt" class="btn">Dataset</a>
        <a href="https://data.ciirc.cvut.cz/public/projects/2022LookForTheChange/resources/poster.pdf" class="btn">Poster</a>
    </header>

    <main id="content" class="main-content" role="main">

<p class="warning">See also our new paper on object state and action discovery <a href="https://soczech.github.io/multi-task-object-states/">here</a>!</p>

<p><img class="overview-image" src="/assets/img/LookForTheChange.svg" alt="Model overview" /></p>

<h1 id="abstract">Abstract</h1>

<p>Human actions often induce changes of object states such as “cutting an apple”, “cleaning shoes” or “pouring coffee”.
In this paper, we seek to temporally localize object states (e.g. “empty” and “full” cup) together with the corresponding state-modifying actions (“pouring coffee”) in long uncurated videos with minimal supervision. The contributions of this work are threefold.
First, we develop a self-supervised model for jointly learning state-modifying actions together with the corresponding object states from an uncurated set of videos from the Internet. The model is self-supervised by the causal ordering signal, i.e. initial object state → manipulating action → end state. 
Second, to cope with noisy uncurated training data, our model incorporates a noise adaptive weighting module supervised by a small number of annotated still images, that allows to efficiently filter out irrelevant videos during training. 
Third, we collect a new dataset with more than 2600 hours of video and 34 thousand changes of object states, and manually annotate a part of this data to validate our approach.
Our results demonstrate substantial improvements over prior work in both action and object state-recognition in video.</p>

<hr />


<h1 id="cvpr-2022-presentation">CVPR 2022 Presentation</h1>

<div class="video">
<iframe src="https://www.youtube.com/embed/Om4lB0tFG5k" width="700" height="480" frameborder="0" allowfullscreen=""></iframe>
</div>

<p>Check out our CVPR 2022 poster <a href="https://data.ciirc.cvut.cz/public/projects/2022LookForTheChange/resources/poster.pdf">here</a>!</p>

<hr />


<h1 id="example-model-predictions">Example Model Predictions</h1>

<div class="video">
<iframe src="https://www.youtube.com/embed/cjW7YEMgVno" width="700" height="480" frameborder="0" allowfullscreen=""></iframe>
</div>

<p>Test it on your videos! Instructions and trained model weights are available at our <a href="https://github.com/soCzech/LookForTheChange">GitHub page</a>.</p>

<hr />



<h1 id="dataset">Dataset</h1>

<p><img class="split-image" src="/assets/img/ChangeIt1.svg" alt="A video from the ChangeIt dataset" /><img class="split-image" src="/assets/img/ChangeIt2.svg" alt="A video from the ChangeIt dataset" /></p>

<p>Information on how to download the <strong>ChangeIt</strong> dataset is available at its <a href="https://github.com/soCzech/ChangeIt">GitHub page</a>.</p>

<hr />



<h1 id="citation">Citation</h1>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">soucek2022lookforthechange</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Sou\v{c}ek, Tom\'{a}\v{s} and Alayrac, Jean-Baptiste and Miech, Antoine and Laptev, Ivan and Sivic, Josef}</span><span class="p">,</span>
    <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
    <span class="na">month</span><span class="p">=</span><span class="s">{June}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<h1 id="acknowledgements">Acknowledgements</h1>

<p>This work was partly supported by the European Regional Development Fund under the project IMPACT (reg. no. CZ.02.1.01/0.0/0.0/15_003/0000468), the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (ID:90140), the French government under management of Agence Nationale de la Recherche as part of the “Investissements d’avenir” program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute), and Louis Vuitton ENS Chair on Artificial Intelligence. We would like to also thank Kateřina Součková and Lukáš Kořínek for their help with the dataset.</p>



      <footer class="site-footer">
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>





